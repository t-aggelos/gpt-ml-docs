# Neural Networks
An intro to neural networks generated by chatGPT.
## The sigmoid function 
The sigmoid function is a common activation function used in neural networks. It maps any real-valued number to a value between 0 and 1. Its mathematical representation is as follows:
### `sigmoid(x) = 1 / (1 + e^(-x))`
 - In the formula, 'e' is the base of the natural logarithm, and 'x' represents the input to the sigmoid function.

### The sigmoid function has an S-shaped curve, and its output values increase gradually from 0 to 1 as the input value increases from negative infinity to positive infinity. The function has the following properties:
- Output Range: The output of the sigmoid function always falls between 0 and 1. When the input is large and positive, the output approaches 1. When the input is large and negative, the output approaches 0. Thus, the sigmoid function can be useful for producing probability-like values or binary classifications.

- Non-linearity: The sigmoid function is nonlinear, which allows neural networks to learn complex relationships between inputs and outputs. Nonlinear activation functions are necessary for neural networks to approximate nonlinear functions effectively.

- Differentiability: The sigmoid function is differentiable, which is important for training neural networks using gradient-based optimization algorithms like backpropagation. The derivative of the sigmoid function can be expressed in terms of the function itself, which simplifies the computation of gradients during backpropagation.

However, it's worth noting that the sigmoid function has some limitations. When the input to the sigmoid function is too large or too small, the function saturates, causing the gradient to be very small. This can lead to slower learning or vanishing gradients, especially in deep neural networks. In practice, other activation functions like ReLU (Rectified Linear Unit) are often preferred for hidden layers in deep neural networks due to their ability to mitigate the vanishing gradient problem.

## The neural network
Neural networks are complex systems that learn to perform tasks by adjusting their internal parameters based on input data. Under the hood, neural networks consist of interconnected artificial neurons (also called nodes or units) organized into layers.

Let's explore the working of neural networks in a simplified manner:
- Input Layer: The input layer receives the initial data or features. Each neuron in the input layer represents a feature of the input data. For example, in an image classification task, each neuron may represent a pixel value.

- Hidden Layers: Neural networks typically have one or more hidden layers between the input and output layers. Each neuron in the hidden layers receives inputs from the previous layer and applies a transformation to produce an output. These transformations are determined by the weights and biases associated with each neuron.

- Weights and Biases: Each connection between neurons in adjacent layers has an associated weight. Weights determine the strength or importance of each input to the neuron. Biases act as additional input values that adjust the output of the neuron. These weights and biases are initially assigned random values and are updated during the training process to optimize the network's performance.

- Activation Function: After applying the transformation (usually a weighted sum of inputs and biases), an activation function is applied to the output of each neuron in the hidden layers. The activation function introduces non-linearity, allowing the neural network to learn complex relationships between inputs and outputs. Common activation functions include sigmoid, ReLU, and tanh.

- Output Layer: The output layer produces the final results of the neural network. The number of neurons in the output layer depends on the nature of the task. For example, in a binary classification problem, the output layer may have a single neuron representing the probability of belonging to one class. In a multi-class classification problem, there will be multiple neurons, each representing the probability of belonging to a specific class.

-  Forward Propagation: Once the input data is fed into the network, the calculations flow forward through the layers in a process called forward propagation. Each neuron receives inputs, applies transformations, and passes the results to the next layer until the final output is generated.

 - Loss Function and Training: To train the neural network, a loss function is used to measure the difference between the predicted output and the desired output. The goal is to minimize this difference by adjusting the weights and biases. This is done using optimization algorithms such as gradient descent and backpropagation. Backpropagation calculates the gradients of the loss with respect to the weights and biases, allowing the network to update them in the direction that reduces the loss.

- Iterative Training: The training process involves repeatedly presenting training examples to the network, adjusting the weights and biases based on the computed gradients, and refining the network's predictions. This iterative process continues until the network's performance reaches a satisfactory level.

By iteratively adjusting the weights and biases through training, neural networks can learn complex patterns and make predictions or classifications based on input data. They can be applied to a wide range of tasks, including image and speech recognition, natural language processing, time series analysis, and much more.

## Weights and Neural Networks
In a neural network, weights are the parameters that define the strength of connections between neurons. They determine how inputs from one layer are combined and transformed to produce outputs in the next layer.
- Each connection between neurons in adjacent layers has an associated weight. These weights represent the importance or contribution of each input to the output of a neuron. Think of them as the "knobs" that the network adjusts during the learning process to achieve the desired output.

- In a feedforward neural network, which is the most common type, weights are typically represented as a matrix. The dimensions of the weight matrix depend on the number of neurons in the current layer (receiving inputs) and the number of neurons in the next layer (producing outputs). Each element of the weight matrix corresponds to the weight value of a specific connection.

During the forward propagation phase, the inputs are multiplied by their corresponding weights and summed up to produce the output of each neuron in the next layer. 
 - The weights determine the strength of these connections and, in turn, influence the output values and overall behavior of the network.

During the training process, the weights are adjusted iteratively using optimization algorithms like backpropagation. 
 - The goal is to find the optimal set of weights that minimize the difference between the network's predictions and the desired outputs for a given set of training examples. This adjustment of weights allows the network to learn and generalize patterns in the data it is trained on.

In summary, weights in a neural network are the parameters that control the strength of connections between neurons, and they play a crucial role in determining the network's ability to learn and make accurate predictions.

## Feed Forwarding
Feedforward is the process in which data flows forward through a neural network, from the input layer through one or more hidden layers to the output layer. It is called "feedforward" because the data moves in a single direction, without any feedback loops or connections that create cycles.

During feedforward propagation, the input data is presented to the network, and computations are performed layer by layer to generate an output. Each layer in the network consists of multiple neurons or units, and these units are interconnected through weighted connections.

Thus, the process of feedforward propagation can be summarized as follows:
- Input Layer: The input data is provided to the network. Each input neuron represents a feature or attribute of the input data.

- Hidden Layers: The input data is transformed and processed through one or more hidden layers. In each hidden layer, the neurons perform computations on the weighted inputs they receive and apply an activation function to produce an output. The output of one layer serves as the input to the next layer, propagating forward through the network.

- Output Layer: The final hidden layer's outputs are propagated to the output layer, which produces the final predictions or outputs of the network. The number of neurons in the output layer depends on the nature of the problem, such as binary classification, multi-class classification, or regression.

- Output Generation: The outputs of the output layer are computed based on the weighted inputs and the activation function specific to the task at hand. For example, in binary classification, a sigmoid or softmax activation function may be used to produce probabilities or class probabilities, respectively.

During feedforward propagation, the weights of the connections between neurons are fixed and determined by the learning process, typically using techniques like gradient descent and backpropagation. The purpose of feedforward propagation is to transform the input data into meaningful representations or predictions in a hierarchical manner.

Once the output is generated, it can be compared to the expected output or target values, and the network's performance can be evaluated using appropriate metrics. The difference between the predicted output and the target values is used to update the weights during the subsequent training process.

In summary, feedforward is the process of forward propagation of data through a neural network, from the input layer to the output layer, involving computation in each layer and the transformation of inputs to produce meaningful outputs or predictions.

### Here is a written example of a scratch implementation of a neural Network class in Javascript:
```
    class NeuralNetwork {
      constructor(inputSize, hiddenSize, outputSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.outputSize = outputSize;
    
        // Initialize weights and biases
        this.weightsInputHidden = this.randomMatrix(hiddenSize, inputSize);
        this.biasHidden = this.randomMatrix(hiddenSize, 1);
        this.weightsHiddenOutput = this.randomMatrix(outputSize, hiddenSize);
        this.biasOutput = this.randomMatrix(outputSize, 1);
      }
    
      // Helper function to initialize a matrix with random values
      randomMatrix(rows, cols) {
        const matrix = [];
        for (let i = 0; i < rows; i++) {
          matrix[i] = [];
          for (let j = 0; j < cols; j++) {
            matrix[i][j] = Math.random() - 0.5; // Random value between -0.5 and 0.5
          }
        }
        return matrix;
      }
    
      // Sigmoid activation function
      sigmoid(x) {
        return 1 / (1 + Math.exp(-x));
      }
    
      // Feedforward propagation
      feedforward(input) {
        // Convert input to matrix format
        input = math.matrix(input);
    
        // Calculate hidden layer output
        let hidden = math.add(math.multiply(this.weightsInputHidden, input), this.biasHidden);
        hidden = math.map(hidden, this.sigmoid);
    
        // Calculate output layer output
        let output = math.add(math.multiply(this.weightsHiddenOutput, hidden), this.biasOutput);
        output = math.map(output, this.sigmoid);
    
        return output.toArray();
      }
    }
```
(I did ask chatGPT this, I apologize in advance for the JavaScript)
### And another in C
```
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

typedef struct {
    int numInputs;
    int numHidden;
    int numOutputs;
    double** weightsInputHidden;
    double** weightsHiddenOutput;
    double* biasHidden;
    double* biasOutput;
} NeuralNetwork;

double sigmoid(double x) {
    return 1.0 / (1.0 + exp(-x));
}

NeuralNetwork* createNeuralNetwork(int numInputs, int numHidden, int numOutputs) {
    NeuralNetwork* nn = malloc(sizeof(NeuralNetwork));
    nn->numInputs = numInputs;
    nn->numHidden = numHidden;
    nn->numOutputs = numOutputs;

    // Allocate memory for weights and biases
    nn->weightsInputHidden = malloc(numHidden * sizeof(double*));
    nn->weightsHiddenOutput = malloc(numOutputs * sizeof(double*));
    nn->biasHidden = malloc(numHidden * sizeof(double));
    nn->biasOutput = malloc(numOutputs * sizeof(double));

    // Initialize weights and biases with random values
    for (int i = 0; i < numHidden; i++) {
        nn->weightsInputHidden[i] = malloc(numInputs * sizeof(double));
        nn->biasHidden[i] = (double)rand() / RAND_MAX - 0.5;
        for (int j = 0; j < numInputs; j++) {
            nn->weightsInputHidden[i][j] = (double)rand() / RAND_MAX - 0.5;
        }
    }

    for (int i = 0; i < numOutputs; i++) {
        nn->weightsHiddenOutput[i] = malloc(numHidden * sizeof(double));
        nn->biasOutput[i] = (double)rand() / RAND_MAX - 0.5;
        for (int j = 0; j < numHidden; j++) {
            nn->weightsHiddenOutput[i][j] = (double)rand() / RAND_MAX - 0.5;
        }
    }

    return nn;
}

double* feedforward(NeuralNetwork* nn, double* input) {
    double* hidden = malloc(nn->numHidden * sizeof(double));
    double* output = malloc(nn->numOutputs * sizeof(double));

    // Calculate hidden layer output
    for (int i = 0; i < nn->numHidden; i++) {
        double sum = nn->biasHidden[i];
        for (int j = 0; j < nn->numInputs; j++) {
            sum += nn->weightsInputHidden[i][j] * input[j];
        }
        hidden[i] = sigmoid(sum);
    }

    // Calculate output layer output
    for (int i = 0; i < nn->numOutputs; i++) {
        double sum = nn->biasOutput[i];
        for (int j = 0; j < nn->numHidden; j++) {
            sum += nn->weightsHiddenOutput[i][j] * hidden[j];
        }
        output[i] = sigmoid(sum);
    }

    free(hidden);
    return output;
}

void destroyNeuralNetwork(NeuralNetwork* nn) {
    for (int i = 0; i < nn->numHidden; i++) {
        free(nn->weightsInputHidden[i]);
    }
    for (int i = 0; i < nn->numOutputs; i++) {
        free(nn->weightsHiddenOutput[i]);
    }
    free(nn->weightsInputHidden);
    free(nn->weightsHiddenOutput);
    free(nn
```

## Common neural network models
- Multilayer Perceptron (MLP): MLP is a basic feedforward neural network model consisting of an input layer, one or more hidden layers, and an output layer. Each layer is composed of interconnected artificial neurons called perceptrons. MLPs are widely used for classification and regression tasks.

- Convolutional Neural Network (CNN): CNN is primarily used for image and video processing. It utilizes specialized layers called convolutional layers that apply filters to input data, enabling the network to automatically learn features from images. CNNs have achieved remarkable success in computer vision tasks such as object recognition and image classification.

- Recurrent Neural Network (RNN): RNN is designed to handle sequential data, where the order of inputs matters. It has recurrent connections that allow information to be propagated through time. RNNs are commonly used in natural language processing (NLP) tasks such as language translation and sentiment analysis.

- Long Short-Term Memory (LSTM): LSTM is an extension of RNN that addresses the vanishing gradient problem and can retain information for longer durations. It introduces memory cells and specialized gates to selectively remember or forget information over time. LSTMs are widely used in tasks that involve long-term dependencies, such as speech recognition and text generation.

- Generative Adversarial Network (GAN): GAN consists of two components: a generator network and a discriminator network. The generator network aims to produce realistic data samples, such as images, while the discriminator network learns to differentiate between real and generated samples. GANs are used for tasks like image synthesis, image-to-image translation, and text generation.

- Reinforcement Learning (RL): RL is a framework where an agent learns to make decisions by interacting with an environment and receiving rewards. Deep Reinforcement Learning (DRL) combines RL algorithms with deep neural networks, enabling agents to learn complex behaviors from raw input data. DRL has achieved impressive results in game playing, robotics, and autonomous systems.

### The Transformer Architcture
The Transformer architecture is a type of neural network architecture introduced in the paper "Attention Is All You Need" by Vaswani et al. in 2017. It has revolutionized the field of natural language processing (NLP) and has been widely adopted in various sequence-based tasks.

The Transformer architecture is based on the concept of self-attention, which allows the model to weigh the importance of different positions within a sequence when processing it. Unlike recurrent neural networks (RNNs) that process sequences sequentially, Transformers can process the entire sequence in parallel, making them more efficient for long-range dependencies.It has been highly successful in NLP tasks such as machine translation, text generation, sentiment analysis, and more. It has demonstrated superior performance and efficiency compared to traditional recurrent-based models, making it a popular choice in the field of sequence modeling and understanding.


Here are some key components and concepts of the Transformer architecture:

- Self-Attention: Self-attention is the core mechanism in Transformers. It allows the model to compute attention weights for each position in the input sequence, based on the relevance of other positions. This enables the model to capture dependencies between different parts of the sequence. Self-attention is computed by forming three matrices: Query, Key, and Value. Attention weights are calculated by taking dot products between the Query and Key matrices.

- Encoder and Decoder: The Transformer architecture consists of an encoder and a decoder. The encoder processes the input sequence and generates a representation - that captures contextual information. The decoder takes this representation and generates an output sequence by attending to the relevant parts of the input.

- Multi-Head Attention: Transformers use multi-head attention, where multiple sets of attention weights are computed in parallel. This allows the model to capture different types of information and attend to multiple aspects of the input simultaneously. The outputs of the attention heads are concatenated and linearly transformed to generate the final attention output.

- Positional Encoding: Since Transformers do not rely on recurrent connections or convolutional filters, they need a way to capture the sequential order of the input. Positional encoding is used to provide the model with information about the position of each element in the sequence. These positional encodings are added to the input embeddings, enabling the model to differentiate between elements based on their positions.

- Feedforward Networks: Transformers also include feedforward neural networks, typically consisting of multiple layers, for each position in the sequence. These networks help capture complex interactions and transformations within the sequence.

- Lastly, a groundbeaking model in AI research that is based on this very architecture is:

## GPT (Generative Pre-trained Transformer) 
 - GPT,for short, is a groundbreaking model in the field of natural language processing (NLP) developed by OpenAI. It is based on the Transformer architecture, which is a type of neural network model specifically designed for sequence tasks.

 - GPT models, such as GPT-3, are pre-trained on large amounts of text data from the internet and can generate human-like text given a prompt or context. These models excel in tasks like language generation, text completion, translation, question-answering, and more.

Some key features and concepts related to GPT models include:

- Transformer Architecture: GPT models are based on the Transformer architecture, which utilizes self-attention mechanisms to capture dependencies between different positions in a sequence. It allows the model to process and understand long-range dependencies effectively.

- Pre-training and Fine-tuning: GPT models are pre-trained in an unsupervised manner on a massive corpus of text data. This pre-training phase helps the model learn language patterns, grammar, and context. After pre-training, the model can be fine-tuned on specific tasks with labeled data to adapt to the target task.

- Attention Mechanism: GPT models employ attention mechanisms to assign different weights to different parts of the input sequence. This mechanism allows the model to focus on relevant words or context while generating responses or making predictions.

- Large-Scale Language Generation: GPT models have an impressive capacity to generate coherent and contextually relevant text. They can produce paragraphs, articles, poetry, and even hold interactive conversations, making them versatile language generation tools.

GPT-3, in particular, is known for its remarkable size, with 175 billion parameters. This large-scale model exhibits impressive language capabilities, but it also demands significant computational resources for training and inference. GPT models have found applications in various domains, including content generation, chatbots, language translation, content summarization, and more. They have contributed significantly to advancing the field of NLP and have opened up exciting possibilities for natural language understanding and generation.

